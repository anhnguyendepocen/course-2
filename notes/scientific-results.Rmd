---
title: "Visualizing scientific results"
output:
  html_document:
    highlight: pygments
    theme: readable
    toc: yes
    toc_float: yes
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

```{r packages, cache = FALSE, message = FALSE}
library(tidyverse)
library(knitr)
library(broom)
library(forcats)
library(stringr)
library(ggrepel)
library(modelr)
library(plotly)

library(socviz)

options(digits = 3)
set.seed(1234)

theme_set(theme_minimal())
```

# Tables or graphs

* Discuss articles
* Debate when to use tables vs graphs
* Paper vs. poster

# Descriptive statistics/exploratory graphs

## Smoothing lines

When examining multivariate continuous data, scatterplots are a quick and easy visualization to assess relationships. However if the data points become too densely clustered, interpreting the graph becomes difficult. Consider the `diamonds` dataset:

```{r diamonds-point}
p <- ggplot(diamonds, aes(carat, price)) +
  geom_point() +
  scale_y_continuous(labels = scales::dollar) +
  labs(x = "Carat size",
       y = "Price")
p
```

What is the relationship between carat size and price? It appears positive, but there are also a lot of densely packed data points in the middle of the graph. **Smoothing lines** are a method for summarizing the relationship between variables to capture important patterns by approximating the functional form of the relationship. The functional form can take on many shapes. For instance, a very common functional form is a **best-fit line**, also known as **ordinary least squares (OLS)** or **simple linear regression**. We can estimate the model directly using `lm()`, or we can directly plot the line by using `geom_smooth(method = "lm")`:

```{r diamonds-lm}
p +
  geom_smooth(method = "lm", se = FALSE)
```

The downside to a linear best-fit line is that it assumes the relationship between the variables is **additive** and **monotonic**. Therefore the summarized relationship between carat size and price seems wildly incorrect for diamonds with a carat size larger than 3. Instead we could use a [**generalized additive model**](http://cfss.uchicago.edu/persp007_nonlinear.html#generalized_additive_models) which allow for flexible, non-linear relationships between the variables while still implementing a basic regression approach:^[`geom_smooth()` automatically implements the `gam` method for datasets with greater than 1000 observations.]

```{r diamonds-gam}
p +
  geom_smooth(se = FALSE)
```

**Locally weighted scatterplot smoothing** (local regression, LOWESS, or LOESS) fits a separate non-linear function at each target point $x_0$ using only the nearby training observations. This method estimates a regression line based on localized subsets of the data, building up the global function $f$ point-by-point.

Here is an example of a local linear regression on the `ethanol` dataset in the `lattice` package:

```{r loess, echo = FALSE, warning = FALSE, message = FALSE}
library(lattice)

mod <- loess(NOx ~ E, ethanol, degree = 1, span = .75)
fit <- augment(mod)

mod0 <- loess(NOx ~ E, ethanol, degree = 0, span = .75)
mod1 <- loess(NOx ~ E, ethanol, degree = 1, span = .75)
mod2 <- loess(NOx ~ E, ethanol, degree = 2, span = .75)

fit_all <- ethanol %>%
  gather_predictions(mod0, mod1, mod2) %>%
  mutate(model = factor(model, levels = c("mod0", "mod1", "mod2"),
                        labels = c("Constant", "Linear", "Quadratic")))

ggplot(fit_all, aes(E, NOx)) +
  geom_point() +
  geom_line(aes(y = pred, color = model)) +
  labs(title = "Local linear regression",
       x = "Equivalence ratio",
       y = "Concentration of nitrogen oxides in micrograms/J",
       color = "Regression")
```

The LOESS is built up point-by-point:

```{r loess_buildup, dependson="loess", fig.show = "animate", echo = FALSE, warning = FALSE, message = FALSE}
dat <- ethanol %>%
  inflate(center = unique(ethanol$E)) %>%
  mutate(dist = abs(E - center)) %>%
  filter(rank(dist) / n() <= .75) %>%
  mutate(weight = (1 - (dist / max(dist)) ^ 3) ^ 3)

library(gganimate)

p <- ggplot(dat, aes(E, NOx)) +
  geom_point(aes(alpha = weight, frame = center)) +
  geom_smooth(aes(group = center, frame = center, weight = weight), method = "lm", se = FALSE) +
  geom_vline(aes(xintercept = center, frame = center), lty = 2) +
  geom_line(aes(y = .fitted), data = fit, color = "red") +
  labs(x = "Equivalence ratio",
       y = "Concentration of nitrogen oxides in micrograms/J")
gganimate(p, interval = .5)
```

One important argument you can control with LOESS is the **span**, or how smooth the LOESS function will become. A larger span will result in a smoother curve, but may not be as accurate. A smaller span will be more local and wiggly, but improve our fit to the training data.

```{r loess_span, dependson="loess", fig.show = "animate", echo = FALSE, warning = FALSE, message = FALSE}
spans <- c(.25, .5, .75, 1)

# create loess fits, one for each span
fits <- data_frame(span = spans) %>%
  group_by(span) %>%
  do(augment(loess(NOx ~ E, ethanol, degree = 1, span = .$span)))

# calculate weights to reproduce this with local weighted fits
dat <- ethanol %>%
  inflate(span = spans, center = unique(ethanol$E)) %>%
  mutate(dist = abs(E - center)) %>%
  filter(rank(dist) / n() <= span) %>%
  mutate(weight = (1 - (dist / max(dist)) ^ 3) ^ 3)

# create faceted plot with changing points, local linear fits, and vertical lines,
# and constant hollow points and loess fit
p <- ggplot(dat, aes(E, NOx)) +
  geom_point(aes(alpha = weight, frame = center)) +
  geom_smooth(aes(group = center, frame = center, weight = weight), method = "lm", se = FALSE) +
  geom_vline(aes(xintercept = center, frame = center), lty = 2) +
  geom_point(shape = 1, data = ethanol, alpha = .25) +
  geom_line(aes(y = .fitted, frame = E, cumulative = TRUE), data = fits, color = "red") +
  facet_wrap(~span) +
  ylim(0, 5) +
  ggtitle("x0 = ") +
  labs(x = "Equivalence ratio",
       y = "Concentration of nitrogen oxides in micrograms/J")

gganimate(p, interval = .5)
```

LOESS lines are best used for datasets with fewer than 1000 observations, otherwise the time and memory usage required to compute the line increases exponentially.

## Coefficient of correlation

* Produces a measure of association, known as Pearson's $r$, that gauges the direction and strength of a relationship between two continuous variables
* Scales between $-1$ and $+1$
  * $-1$ -- perfect negative association between the variables
  * $+1$ -- perfect positive association between the variables
  * $0$ -- no relationship between the variables
* Unit-less measure - no matter what scale the variables fall on (e.g. turnout, education, income), the number will always fall between $-1$ and $+1$

```{r pearson-r}
r_plot <- function(r, n = 100){
  xy <- ecodist::corgen(len = n, r = r) %>%
    bind_cols
  
  ggplot(xy, aes(x, y)) +
    geom_point() +
    ggtitle(str_c("Pearson's r = ", r))
}

r <- c(.8, 0, -.8)

for(r in r){
  print(r_plot(r))
}
```

## Scatterplot matricies

To quickly visualize several variables in a dataset and their relation to one another, a **scatterplot matrix** is a quick and detailed tool for generating a series of scatterplots for each combination of variables. Consider `credit.csv` which contains a sample of individuals from a credit card company, identifying their total amount of credit card debt and other financial/demographic variables:

```{r credit-import}
credit <- read_csv("data/Credit.csv") %>%
  # remove first ID column
  select(-X1)
names(credit) <- stringr::str_to_lower(names(credit))   # convert column names to lowercase
str(credit)
```

If we want to quickly assess the relationship between all of the variables (in preparation for more advanced statistical learning techniques), we could generate a matrix of scatterplots using the base `graphics` package:

```{r credit-scatter-matrix}
pairs(select_if(credit, is.numeric))
```

* This only works well if we use strictly quantitative variables (hence the use of `select_if()`)
* We don't automatically see correlation information
* It's not built using `ggplot2` so it's hard to modify using techniques with which we are already familiar

Instead, we can use `GGally::ggpairs()` to generate a scatterplot matrix. `GGally` is a package for R that extends `ggplot2` by adding helper functions for common multivariate data structures. `ggpairs()` is a function that allows us to quickly generate a scatterplot matrix.

```{r credit-scatter-ggpairs}
library(GGally)

ggpairs(select_if(credit, is.numeric))
```

When applied to strictly numeric variables, the lower triangle generates scatterplots, the upper triangle prints the correlation coefficient, and the diagonal panels are density plots of the variable.

Because `ggpairs()` is ultimately based on `ggplot()`, we can use the same types of commands to modify the graph. For instance, if we want to use the color aesthetic to distinguish between men and women in the dataset:

```{r credit-scatter-matrix-gender}
ggpairs(credit, mapping = aes(color = gender),
        columns = c("income", "limit", "rating", "cards", "age", "education", "balance"))
```

Or if we wanted to draw a smoothing line instead of scatterplots, we can modify the graph's [matrix sections](http://ggobi.github.io/ggally/index.html#matrix_sections):

```{r credit-scatter-smoother}
ggpairs(select_if(credit, is.numeric),
        lower = list(
          continuous = "smooth"
        )
)
```

Hmm, too difficult to see the smoothers because the points are so dense. We can use `wrap()` to pass through individual parameters to the underlying `geom_()`:

```{r wrap}
ggpairs(select_if(credit, is.numeric),
        lower = list(
          continuous = wrap("smooth", alpha = .1, color = "blue")
        )
)
```

Or we can write a custom function and apply it to the lower triangle panels:

```{r credit-scatter-point-smoother, message = FALSE, warning = FALSE}
scatter_smooth <- function(data, mapping, ...) {
  ggplot(data = data, mapping = mapping) +
    # make data points transparent
    geom_point(alpha = .2) +
    # add default smoother
    geom_smooth(se = FALSE)
}

ggpairs(select_if(credit, is.numeric),
        lower = list(
          continuous = scatter_smooth
        )
)

ggpairs(credit, mapping = aes(color = gender),
        columns = c("income", "limit", "rating", "cards", "age", "education", "balance"),
        lower = list(
          continuous = scatter_smooth
        )
)
```

`ggpairs()` also works on datasets with a mix of qualitative and quantitative variables, drawing appropriate graphs based on whether the variables are continuous or discrete:

```{r diamonds-scatter-matrix, warning = FALSE, message = FALSE}
rcfss::scorecard %>%
  select(type:debt) %>%
  ggpairs
```

## Heatmap of correlation coefficients

Scatterplot matricies can provide lots of information, but can also be very densely packed. Perhaps instead we want to quickly visualize the correlation between each of the variables.^[Example drawn from [ ggplot2 : Quick correlation matrix heatmap - R software and data visualization](http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization).] We can easily calculate the correlation coefficients using `cor()`:

```{r mpg-cor}
(mpg_lite <- select_if(mpg, is.numeric))

(cormat <- mpg_lite %>%
  cor %>%
  round(2))
```

But who likes yucky tables. Instead let's turn this into a heatmap. First we need to reshape the data into a tidy structure:

1. Each row contains a single observation
1. Each column contains a single variable
1. Each cell contains a single value

What we need is a data frame with three columns:

1. First variable name
1. Second variable name
1. Correlation coefficient

We can use `reshape2::melt()` to quickly accomplish this:

```{r cormat-tidy}
library(reshape2)
(melted_cormat <- melt(cormat))
```

We can then use `geom_tile()` to visualize the correlation matrix:

```{r cormat-tile}
ggplot(melted_cormat, aes(x = Var1, y = Var2, fill = value)) + 
  geom_tile()
```

Not exactly pretty. We can clean it up first by reducing redundancy (remember the upper and lower triangles provide duplicate information):

```{r cormat-reduce}
# Get lower triangle of the correlation matrix
get_lower_tri<-function(cormat){
  cormat[upper.tri(cormat)] <- NA
  return(cormat)
}

# Get upper triangle of the correlation matrix
get_upper_tri <- function(cormat){
  cormat[lower.tri(cormat)]<- NA
  return(cormat)
}

upper_tri <- get_upper_tri(cormat)
upper_tri
```

Now melt `upper_tri` and repeat the same process, cleaning up the colors for the heatmap as well to distinguish between positive and negative coefficients:

```{r cormat-tile-tidy}
melted_cormat <- melt(upper_tri, na.rm = TRUE)

ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white") +
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Pearson\nCorrelation") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1)) +
 coord_fixed()
```

We can also reorder the correlation matrix according to correlation coefficient to help reveal additional trends:

```{r reorder-cormat}
reorder_cormat <- function(cormat){
  # Use correlation between variables as distance
  dd <- as.dist((1-cormat)/2)
  hc <- hclust(dd)
  cormat <-cormat[hc$order, hc$order]
}

# Reorder the correlation matrix
cormat <- reorder_cormat(cormat)
upper_tri <- get_upper_tri(cormat)

# Melt the correlation matrix
melted_cormat <- melt(upper_tri, na.rm = TRUE)

# Create a ggheatmap
ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
  geom_tile(color = "white")+
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1,1), space = "Lab", 
                       name="Pearson\nCorrelation") +
  theme_minimal()+ # minimal theme
  theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                   size = 12, hjust = 1))+
  coord_fixed()

# Print the heatmap
print(ggheatmap)
```

Finally we can directly label the correlation coefficient values on the graph, so we have both the color channel and exact values:

```{r add-coef-heatmap}
ggheatmap + 
  geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    axis.ticks = element_blank(),
    legend.position = "bottom")
```

To make it more flexible, we can also turn all of this into a function that works for any dataset:

```{r coef-heatmap-function}
cormat_heatmap <- function(data){
  # generate correlation matrix
  cormat <- round(cor(data), 2)
  
  # melt into a tidy table
  get_upper_tri <- function(cormat){
    cormat[lower.tri(cormat)]<- NA
    return(cormat)
  }
  
  upper_tri <- get_upper_tri(cormat)
  
  # reorder matrix based on coefficient value
  reorder_cormat <- function(cormat){
    # Use correlation between variables as distance
    dd <- as.dist((1-cormat)/2)
    hc <- hclust(dd)
    cormat <-cormat[hc$order, hc$order]
  }
  
  cormat <- reorder_cormat(cormat)
  upper_tri <- get_upper_tri(cormat)
  
  # Melt the correlation matrix
  melted_cormat <- melt(upper_tri, na.rm = TRUE)
  
  # Create a ggheatmap
  ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
    geom_tile(color = "white")+
    scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                         midpoint = 0, limit = c(-1,1), space = "Lab", 
                         name="Pearson\nCorrelation") +
    theme_minimal()+ # minimal theme
    theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                     size = 12, hjust = 1))+
    coord_fixed()
  
  # add correlation values to graph
  ggheatmap + 
    geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
    theme(
      axis.title.x = element_blank(),
      axis.title.y = element_blank(),
      panel.grid.major = element_blank(),
      panel.border = element_blank(),
      panel.background = element_blank(),
      axis.ticks = element_blank(),
      legend.position = "bottom")
}

cormat_heatmap(select_if(mpg, is.numeric))
cormat_heatmap(select_if(credit, is.numeric))
cormat_heatmap(select_if(diamonds, is.numeric))
```

## Parallel coordinate plots

**Parallel coordinate plots** are an alternative graphical format for multivariate data analysis (continuous or discrete). They can be quite busy and messy. Key things for parallel coordinate plots:

* Ordering variables in different ways helps to identify relevant patterns. Therefore a lot of this is trial and error
* Adding interactivity (as we will see in later weeks) helps

```{r pcp}
ggparcoord(data = iris, columns = 1:4, groupColumn = 5)

# with the iris data, order the axes by overall class (Species) separation
# using the anyClass option
ggparcoord(data = iris, columns = 1:4, groupColumn = 5, order = "anyClass")

# add points to the plot, add a title, and use an alpha scalar to make the
# lines transparent
p <- ggparcoord(data = iris, columns = 1:4, groupColumn = 5, order = "anyClass", 
    showPoints = TRUE, title = "Parallel Coordinate Plot for the Iris Data", 
    alphaLines = 0.3)
p

# add some basic interactivity
ggplotly(p)
```

## Three-dimensional plots

Adding a third (or fourth) dimension to a two-dimensional plot is relatively trivial when at least one of the variables is discrete:

```{r add-dimensions}
ggplot(mpg, aes(displ, hwy, color = class)) +
  geom_point()
```

However what happens when you have three continuous dimensions to represent? Can we draw 3D graphs in R? Not easily, and interpreting 3D graphs can also be challenging mentally. `ggplot2` cannot draw graphs in three dimensions. One possibility is to keep the data in two physical dimensions by using `geom_tile()` and adding the third dimension using the fill aesthetic (color channel). For example, say we estimate a logistic regression model of the probability of voting in the 1996 US presidential election and we want to visualize the predicted probability of survival for each combination of these variable:

```{r vote96-model}
# import data
(vote <- rcfss::mental_health)

# estimate model
vote_glm <- glm(vote96 ~ age + educ, data = vote, family = "binomial")
tidy(vote_glm)

# extract predicted probabilities
vote_prob <- vote %>%
  data_grid(age = 18:89, educ = 0:20) %>%
  modelr::add_predictions(vote_glm) %>%
  # convert predicted values to probabilities
  mutate(prob = rcfss::logit2prob(pred))

ggplot(vote_prob, aes(age, educ, fill = prob)) +
  geom_tile() +
  scale_fill_gradient2(midpoint = .5, label = scales::percent) +
  labs(title = "Probability of voter turnout in 1996",
       x = "Age",
       y = "Education (in years)",
       fill = "Probability\nof voting")

# cleaner image using geom_raster and interpolate = TRUE
ggplot(vote_prob, aes(age, educ, fill = prob)) +
  geom_raster(interpolate = TRUE) +
  scale_fill_gradient2(midpoint = .5, label = scales::percent) +
  labs(title = "Probability of voter turnout in 1996",
       x = "Age",
       y = "Education (in years)",
       fill = "Probability\nof voting")
```

### `persp()`


### `plot3d`

http://www.sthda.com/english/wiki/impressive-package-for-3d-and-4d-graph-r-software-and-data-visualization

### `plotly`

If we wanted to represent this in true three-dimensional fashion, we could use [plotly](https://plot.ly/):

```{r 3d-vote}
plot_ly(vote_prob, x = ~age, y = ~educ, z = ~prob) %>%
  add_mesh()
```

```{r 3d-credit}
plot_ly(credit, x = ~limit, y = ~balance, z = ~income) %>%
  add_mesh()
```

## 3D surface plot

```{r 3d-surface}
plot_ly(z = ~volcano) %>% add_surface()

volcano %>%
  melt %>%
  ggplot(aes(Var1, Var2, z = value)) +
  geom_contour(aes(color = ..level..))
```

## Mosaic plot

What is the relationship between happiness and gender? We could identify this in several different contingency tables, depending on the probability distribution on which we wish to focus:

```{r mosaic-happy}
# Mosaic plot of happiness and education
library(productplots)
data("happy")

happy <- happy %>%
  na.omit
```

### Joint distribution

* $f(\text{happy}, \text{sex})$

```{r happy-sex}
# contingency table
library(descr)
crosstab(happy$happy, happy$sex, prop.t = TRUE, plot = FALSE)
```

### Conditional distribution of sex given happiness

* $f(\text{sex} | \text{happy})$
* $f(\text{happy} | \text{sex})$

```{r happy-sex-cond}
crosstab(happy$happy, happy$sex, prop.r = TRUE, plot = FALSE)
crosstab(happy$happy, happy$sex, prop.c = TRUE, plot = FALSE)
```

### Conditional distribution of happiness given sex and marginal distribution of sex

* $f(\text{happy})$ and $f(\text{sex})$

```{r happy-sex-marg}
crosstab(happy$happy, happy$sex, prop.c = TRUE, prop.r = TRUE, plot = FALSE)
```

Each of the contingency tables encourages a different type of comparison, therefore the author has to decide in advance which comparison is most important and include the appropriate table. Alternatively, we can visualize this information using a **mosaic plot**, whereby the area of each rectangle is proportional to the number of observations falling into the respective contengencies.

There are a few different packages available for drawing mosaic plots in R.

### `graphics::mosaicplot()`

```{r graphics-mosaicplot}
mosaicplot(~ sex + happy, data = happy)
```

### `vcd::mosaic()`

```{r vcd-mosaic}
library(vcd)
mosaic(~ happy + sex, happy)
```

### `productplots::prodplot()`

* Developed by Hadley Wickham and based on `ggplot2`
* [GitHub repo](https://github.com/hadley/productplots)
* Based on a ["product plots" framework](http://vita.had.co.nz/papers/prodplots.pdf) for visualizing tables of counts, proportions, and probabilities

```{r productplots}
# mosaic plot using productplots
prodplot(happy, ~ happy + sex)

# add color
prodplot(happy, ~ happy + sex) +
  aes(fill = happy) +
  theme(panel.grid = element_blank())

prodplot(happy, ~ happy + marital) +
  aes(fill = happy) +
  theme(legend.position = "none") +
  theme(panel.grid = element_blank())
```

Notice that the mosaic plot is very similar to a proportional bar chart:

```{r prop-barchart}
ggplot(happy, aes(marital, fill = happy)) +
  geom_bar(position = "fill")
```

However unlike a proportional bar chart, the bar widths are constant and therefore we do not know what proportion of individuals in the survey are married vs. never married, or any other similar comparison.

## Dot plot for summary statistics

```{r oj}
library(ISLR)

OJ_sum <- OJ %>%
  select(ends_with("MM"), ends_with("CH")) %>%
  gather(var, value) %>%
  group_by(var) %>%
  summarize(mean = mean(value),
            sd = sd(value),
            min = min(value),
            max = max(value),
            n = n())

# print the table
kable(OJ_sum)

# plot using a single dot plot
ggplot(OJ_sum, aes(x = fct_reorder(var, mean), y = mean)) +
  geom_linerange(aes(ymin = mean - 2 * sd,
                      ymax = mean + 2 * sd),
                  linetype = 2,
                 size = .25) +
  geom_linerange(aes(ymin = mean - sd,
                      ymax = mean + sd),
                  size = 1) +
  geom_point() +
  coord_flip() +
  labs(x = NULL,
       y = NULL)

# dodge based on OJ brand
OJ_sum %>%
  separate(var, into = c("var", "brand"), -3, remove = TRUE) %>%
  ggplot(aes(x = fct_reorder(var, mean), y = mean, color = brand)) +
  geom_linerange(aes(ymin = mean - 2 * sd,
                      ymax = mean + 2 * sd),
                  linetype = 2,
                 size = .25,
                 position = position_dodge(width = 0.5)) +
  geom_linerange(aes(ymin = mean - sd,
                      ymax = mean + sd),
                  size = 1,
                 position = position_dodge(width = 0.5)) +
  geom_point(position = position_dodge(width = 0.5)) +
  coord_flip() +
  labs(x = NULL,
       y = NULL,
       color = "Brand")

# facet based on OJ brand
OJ_sum %>%
  separate(var, into = c("var", "brand"), -3, remove = TRUE) %>%
  ggplot(aes(x = fct_reorder(var, mean), y = mean)) +
  facet_grid(. ~ brand) +
  geom_linerange(aes(ymin = mean - 2 * sd,
                      ymax = mean + 2 * sd),
                  linetype = 2,
                 size = .25) +
  geom_linerange(aes(ymin = mean - sd,
                      ymax = mean + sd),
                  size = 1) +
  geom_point() +
  coord_flip() +
  labs(x = NULL,
       y = NULL,
       color = "Brand")
```

# Model results


## `broom`


## `stargazer`


## Extracting model objects


## Grouped analysis and list columns


## Plot marginal effects



# Session Info {.toc-ignore}

```{r cache = FALSE}
devtools::session_info()
```
